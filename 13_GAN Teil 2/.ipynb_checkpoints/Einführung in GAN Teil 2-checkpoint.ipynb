{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN - Teil 2 - Keras kann auch Ziffern schreiben\n",
    "\n",
    "## Worum geht es in diesem Teil\n",
    "Nachdem ich im ersten Teil den grundlegenden Trick der GAN beschrieben habe, m√∂chte ich hier ein einfaches GAN bauen, dass in der Lage ist, die handschriftlichen Ziffern aus der MNIST Datenbank t√§uschend echt nachzumachen.\n",
    "Ich verwende f√ºr diesen ‚ÄûZaubertrank‚Äú nur die elementaren Zutaten aus der Speisekammer der neuronalen Netze um mich auf die GAN Spezifika konzentrieren zu k√∂nnen. Nichts desto Trotz ist das Ergebnis schon recht beeindruckend. Wenn ich hier von Zaubertrank und Speisekammer rede, so geschieht das nicht ganz ohne Grund. In der Literatur wird gerne der Ausdruck \"Alchemie der GAN\" verwendet, der darauf hinweisen soll, dass man viel Erfahrung und Gesp√ºr beim Bau von GAN's ben√∂tigt. Das liegt daran, dass wir mit unseren neuronalen Netzen mittels Gradientenverfahren nicht einem festen Minima entgegenschreiten, sondern wir suchen ein Gleichgewicht zwischen G(z) und D(x). Dadurch verschieben sich die Minima in den Netzten nach jedem Iterationsschritt. Die Aufgabe beim Bau von GAN's ist es also seine Netzte so zu bauen, in dem das Gleichgewicht leicht zu finden ist. Man braucht ein Netz wie eine tiefe Sch√ºssel, in der eine kleine Kugel automatisch dem tiefsten Punkt und damit seinem Gleichgewicht zustrebt. Das andere Extrem ist ein Netz wie ein Bleistift, den man auf der Spitze ausbalancieren m√∂chte, was bekanntlich schnell mal frustrieren kann. \n",
    "H√§ngen wir uns also unsere Druidenm√§ntel um und greifen zu unseren goldenen Sicheln üòä\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die l√§stigen Vorbereitungen\n",
    "Wie immer sammle ich alle n√∂tigen Importstatements in einem Block \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zY5grVFnoFdV",
    "outputId": "dd8533c6-aa7a-454a-ee4a-57ca6e8902fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version:\t1.10.0\n",
      "Keras Version:\t\t2.2.4\n",
      "Found GPU & CPU devices:\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9164342101682752304\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4960052838\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 11323770166586016644\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras import initializers\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "print(\"TensorFlow Version:\\t{}\".format(tf.__version__))\n",
    "print(\"Keras Version:\\t\\t{}\".format(keras.__version__))\n",
    "print(\"Found GPU & CPU devices:\\n{}\".format(device_lib.list_local_devices()))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ein h√ºbsches Tool f√ºr Progerss Bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# F√ºr das Rauschen brauchen wir Zufallszahlen\n",
    "np.random.seed(911972)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten vorbereiten\n",
    "\n",
    "* Zuerst besorge ich mir die Testdaten auf dem √ºblichen Weg. \n",
    "* Da ich hier wirklich die einfachsten Netze verwenden m√∂chte (also simple Dense Netze), sammle ich alle Pixelinformationen nicht in einer zwei dimensionalen 28 x 28 Matrix, sondern in einem 28 x 28 = 784 Elemente gro√üen Vektor.\n",
    "* Ein erster Erfahrungswert ist es, den Tangens Hyperbolicus als finale Aktivierungsfunktion zu verwenden. Deshalb normiere ich meine Daten auf das Intervall von [-1, 1].\n",
    "* Die Gr√∂√üe des Input Vektors f√ºr den Generator setzte ich Zuf√§llig auf 97\n",
    "* Die Optimierung der Hyperparameter ist die wesentliche Flei√üarbeit. Somit sammle ich sie hier in einem Block zusammen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcQ3qQ-QpQIW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Rohdaten:\n",
      "[  0   0   0  81 240 253 253 119  25   0]\n",
      "2. Zum Vektor aufgebogen:\n",
      "[  0   0   0  81 240 253 253 119  25   0]\n",
      "3. in das Interval [-1, 1] normiert:\n",
      "[-1.         -1.         -1.         -0.36470586  0.88235295  0.9843137\n",
      "  0.9843137  -0.06666666 -0.8039216  -1.        ]\n"
     ]
    }
   ],
   "source": [
    "(real_images, __), (__, __) = mnist.load_data()\n",
    "print('1. Rohdaten:')\n",
    "print(real_images[0, 14, 10:20])\n",
    "\n",
    "real_images = real_images.reshape(60000, 784)\n",
    "print('2. Zum Vektor aufgebogen:')\n",
    "print(real_images[0, 14*28 +10 : 14*28 + 20])\n",
    "\n",
    "real_images = real_images.astype('float32')  / 255 * 2 - 1\n",
    "print('3. in das Interval [-1, 1] normiert:')\n",
    "print(real_images[0, 14*28 +10 : 14*28 + 20])\n",
    "\n",
    "\n",
    "noise_vector_size = 97\n",
    "\n",
    "# Da ich f√ºr alle Netzte einen Optimizer brauche, der ein wenig sanfter als der Default eingestellt ist, monfiguriere ich ihn einmal\n",
    "#optimizer= Adam(lr=0.0002, beta_1=0.5)\n",
    "optimizer= RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8)\n",
    "\n",
    "# dropout ist wichtig bei GAN\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# die steigung der LeakyReLu im negativen Bereich\n",
    "leaky_faktor = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der Generator\n",
    "Im Prinzip ist der Generator das einfachste Netz, das man bauen kann. Eine kleine Besonderheit hat es aber dennoch. Ich verwende hier ‚ÄöLeakyReLU‚Äò als Ansatzfunktion. Seit einiger Zeit setzt sich ReLU als Allzweckwaffe unter den Ansatzfunktionen der Hidden - Layer durch. Allerdings hat sie eine kleine Schw√§che. Dadurch, dass sie den negativen Wertebereich auf Null abbildet, l√§sst sie Knoten schnell absterben. Ein einmal abgestorbener Knoten wird nicht mehr aktiviert, da auf ihn kein Fehler mehr aufgeteilt wird. Das wirkt sich negativ auf das ‚Äûtiefe‚Äú propagieren von Fehlern aus. Mit der LeakyReLu wird dieses Problem repariert, indem man den negativen Wertebereich nicht ganz auf 0 abbildet. \n",
    "\n",
    "Als Aktivierungsfunktion des letzten Layers w√§hle ich wie bereits erw√§hnt den tanh als ‚Äûmagische‚Äú Zutat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldJsnt31pQzV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               25088     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 784)               803600    \n",
      "=================================================================\n",
      "Total params: 1,485,584\n",
      "Trainable params: 1,485,584\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = Sequential()\n",
    "generator.add(Dense(256, input_dim=noise_vector_size, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "generator.add(LeakyReLU(leaky_faktor))\n",
    "\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(leaky_faktor))\n",
    "\n",
    "generator.add(Dense(1024))\n",
    "generator.add(LeakyReLU(leaky_faktor))\n",
    "\n",
    "generator.add(Dense(784, activation='tanh'))\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "generator.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Der Diskriminator\n",
    "Der Diskriminator ist √§hnlich schlicht aufgebaut, wie der Generator. Die Input Dimension ist 28 x 28, also jeweils ein Bild. Die Output Dimension ist ein Skalar zwischen Null und Eins.\n",
    "Der Diskriminator verf√ºgt anders als der Generator √ºber Dropout Layer, um auch hier den Zufall mit einzubinden. Der Zufall verhindert im GAN, dass sich die Netze zu schnell einschn√ºren und in einen d√ºnn besiedelten Zustand gleiten. Das ist bei GAN‚Äôs sehr schlecht, da die ‚ÄûKreativit√§t‚Äú in der Vielfalt der M√∂glichkeiten liegt. Somit m√ºssen m√∂glichst viele Gewichte aktiviert sein um eine m√∂glichst breite Varianz zu bekommen. Der Diskriminator bekommt seine Vielfalt √ºbrigens nicht durch Dropout Layer, sondern durch das Rauschen des Input Vektors z.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,460,225\n",
      "Trainable params: 1,460,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Sequential()\n",
    "discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "discriminator.add(LeakyReLU(leaky_faktor))\n",
    "discriminator.add(Dropout(dropout_rate))\n",
    "\n",
    "discriminator.add(Dense(512))\n",
    "discriminator.add(LeakyReLU(leaky_faktor))\n",
    "discriminator.add(Dropout(dropout_rate))\n",
    "\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(leaky_faktor))\n",
    "discriminator.add(Dropout(dropout_rate))\n",
    "\n",
    "discriminator.add(Dense(1, activation='sigmoid'))\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Das GAN\n",
    "Um den Generator trainieren zu k√∂nnen, muss man das GAN zusammensetzen.  Dazu nutzt man bei Keras das funktionale API. Damit kann man einem Model weitere Layer hinzuf√ºgen. So wird hier der Input Layer auf den Generator und dieser auf den Diskriminator geschichtet. Danach ruft man den Model Konstruktor auf, um das GAN zu bekommen.\n",
    "√úbrigens spiegelt sich in der funktionalen Schreibweise von Python die theoretische Herleitung der GAN‚Äôs wieder D(G(z)) .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WDCt1dFpVxy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 97)                0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 784)               1485584   \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 1)                 1460225   \n",
      "=================================================================\n",
      "Total params: 2,945,809\n",
      "Trainable params: 1,485,584\n",
      "Non-trainable params: 1,460,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# der Diskriminator wird hier gleich eingefroren\n",
    "discriminator.trainable = False\n",
    "\n",
    "# der verrauschte Input Vektor\n",
    "z = Input(shape=(noise_vector_size,))\n",
    "\n",
    "# Das GAN D(G(z))\n",
    "gan_core = discriminator(generator(z))\n",
    "\n",
    "# Um ein eigenst√§ndiges Netz zu erhalten, baut man sich ein neues Model\n",
    "gan = Model(inputs = z, outputs = gan_core)\n",
    "gan.compile(loss = 'binary_crossentropy', optimizer = optimizer)\n",
    "gan.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eThr0UwpZ5q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 1 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:27<00:00, 33.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 2 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:20<00:00, 46.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 3 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:18<00:00, 51.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 4 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:20<00:00, 52.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 5 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:17<00:00, 52.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 6 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:18<00:00, 49.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 7 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:20<00:00, 45.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 8 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:19<00:00, 49.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 9 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:19<00:00, 48.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 10 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:20<00:00, 45.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 11 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:20<00:00, 44.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 12 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:19<00:00, 54.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 13 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:17<00:00, 53.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 14 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:17<00:00, 54.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 15 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:16<00:00, 60.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 16 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:15<00:00, 59.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 17 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:17<00:00, 55.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 18 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:17<00:00, 56.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 19 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:16<00:00, 58.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 20 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:16<00:00, 55.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 21 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:17<00:00, 53.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 22 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:17<00:00, 53.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 23 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:17<00:00, 50.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 24 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:18<00:00, 51.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 25 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:19<00:00, 48.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 26 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:18<00:00, 51.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 27 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:19<00:00, 53.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 28 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:18<00:00, 51.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 29 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 937/937 [00:27<00:00, 33.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Epoch 30 ---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 618/937 [00:13<00:06, 51.08it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 5000\n",
    "batch_size = 128\n",
    "batch_count = real_images.shape[0] / batch_size\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    print('-'*15, 'Epoch %d' % e, '-'*15)\n",
    "    for _ in tqdm(range(int(batch_count))):\n",
    "        # Get a random set of input noise and images\n",
    "        noise = np.random.normal(0, 0.1, size=[batch_size, noise_vector_size])\n",
    "        image_batch = real_images[np.random.randint(0, real_images.shape[0], size=batch_size)]\n",
    "\n",
    "        # Generate fake MNIST images\n",
    "        generated_images = generator.predict(noise)\n",
    "        X = np.concatenate([image_batch, generated_images])\n",
    "\n",
    "        # Labels for generated and real data\n",
    "        y_dis = np.zeros(2*batch_size)\n",
    "        # One-sided label smoothing\n",
    "        y_dis[:batch_size] = 0.9\n",
    "\n",
    "        # Train discriminator\n",
    "        discriminator.trainable = True\n",
    "        discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "        # Train generator\n",
    "        noise = np.random.normal(0, 1, size=[batch_size, noise_vector_size])\n",
    "        y_gen = np.ones(batch_size)\n",
    "        discriminator.trainable = False\n",
    "        gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "    if e == 1 or e % 20 == 0:\n",
    "            examples = 100\n",
    "            dim=(10, 10)\n",
    "            figsize=(10, 10)\n",
    "            noise = np.random.normal(0, 1, size=[examples, noise_vector_size])\n",
    "            generated_images = generator.predict(noise)\n",
    "            generated_images = generated_images.reshape(examples, 28, 28)\n",
    "\n",
    "            plt.figure(figsize=figsize)\n",
    "            for i in range(generated_images.shape[0]):\n",
    "                plt.subplot(dim[0], dim[1], i+1)\n",
    "                plt.imshow(generated_images[i], interpolation='nearest', cmap='gray_r')\n",
    "                plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('output/gan_generated_image_epoch_%d.png' % e)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "intro_to_gans.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
